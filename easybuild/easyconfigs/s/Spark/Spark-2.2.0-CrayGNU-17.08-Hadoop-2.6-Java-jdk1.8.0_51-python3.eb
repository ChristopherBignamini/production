easyblock = 'Tarball'

name = 'Spark'
version = '2.2.0'

py_maj_ver = '3'
py_min_ver = '6'
py_rev_ver = '1.1'

pyver = '%s.%s' % (py_maj_ver, py_min_ver)
javaver = 'jdk1.8.0_51'

versionsuffix = '-Hadoop-2.6-Java-%s-python%sa' % (javaver, py_maj_ver)

homepage = 'http://spark.apache.org'
description = """Spark is Hadoop MapReduce done in memory"""

toolchain = {'name': 'CrayGNU', 'version': '17.08'}

patches = [('patch-file.patch', 1)]

sources = ['%(namelower)s-%(version)s-bin-hadoop2.6.tgz']
source_urls = [
    'http://apache.belnet.be/%(namelower)s/%(namelower)s-%(version)s/',
    'http://www.eu.apache.org/dist/%(namelower)s/%(namelower)s-%(version)s/',
    'http://www.us.apache.org/dist/%(namelower)s/%(namelower)s-%(version)s/',
]
checksums = ['86f33f6f4fe545cb848600004144f09b6e8413af47ce02c1203c3ecda075288d']

dependencies = [
    ('cray-python/3.6.1.1', EXTERNAL_MODULE),
    ('java/%s' % javaver, EXTERNAL_MODULE),
]

prebuildopts = "export SLURM_JOBID=11111"
postinstallcmds = [("export SLURM_JOBID=11111")]
#modextravars = {'SLURM_JOBID': 1} 

exts_defaultclass = 'PythonPackage'

#exts_list = [
#    ('py4j', '0.10.4', {
#        'req_py_majver': '3',
#        'req_py_minver': '6',
#        'prebuildopts': "export SLURM_JOBID=11111",
#        'postinstallcmds': [("export SLURM_JOBID=11111")],
#        'source_urls': ['https://pypi.python.org/packages/source/p/py4j'],
#        'checksums': ['b49a28c6bd96d334a39c15c3ce52225585332eff5e2659c7dc7aa223aa8bd454'],
#    }),
#    ('pyspark', version, {
#        'req_py_majver': '3',
#        'req_py_minver': '6',
#        'source_tmpl': 'spark-%(version)s-bin-hadoop2.6.tgz',
#        'start_dir': 'python',
#        'checksums': ['86f33f6f4fe545cb848600004144f09b6e8413af47ce02c1203c3ecda075288d'],
#    }),
#]

modextrapaths = {'PYTHONPATH': ['lib/python%s/site-packages' % (pyver)]}

sanity_check_paths = {
    'files': ['bin/spark-shell', 'bin/pyspark'],
    'dirs': ['python']
}

moduleclass = 'devel'

modtclfooter = """
## Required internal variables
set name     spark
set version  2.2.0
set sparkbin  $root/cscs-custom 

## Required for SVN hook to generate SWDB entry
set     fullname    Spark
set     externalurl http://spark.apache.org
set     nerscurl    http://spark.apache.org
set     maincategory    applications
set     subcategory debugging
set     description "Spark data analytic framework"

prepend-path PATH $sparkbin:$root
if {! [info exists env(SLURM_JOBID) ] } {
  puts stderr "SLURM_JOBID not set, please run this module inside of a batch job"
  exit
}

set workerdir $env(PWD)/sparkjob.$env(SLURM_JOBID)
set nodefile $env(SLURM_JOB_NODELIST)

setenv SPARK_WORKER_DIR $workerdir
setenv SPARK_SLAVES $workerdir/slaves
setenv SPARK_LOG_DIR $workerdir/sparklogs
setenv SPARK_LOCAL_DIRS "/tmp"
setenv SPARK_PREFIX $root
setenv SPARK_CONF_DIR $workerdir/conf

if { [ module-info mode load ] } {
    puts stderr "Creating Directory SPARK_WORKER_DIR $env(SPARK_WORKER_DIR)"
    puts stderr "Creating $env(SPARK_WORKER_DIR)/slaves file"
    puts stderr "Determining the master node name..."
    set master [exec $sparkbin/getmaster.sh]
    puts stderr "Master node is $master"

    exec /bin/mkdir -p $env(SPARK_WORKER_DIR)
    exec  $sparkbin/getslaves.sh $env(SPARK_WORKER_DIR)/slaves
    setenv SPARKURL spark://$master:7077
    setenv SPARKMASTER $master
}
"""
